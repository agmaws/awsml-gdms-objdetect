{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Object Detection Models in SageMaker with Augmented Manifests\n",
    "\n",
    "This notebook demonstrates the use of an \"augmented manifest\" to train an object detection machine learning model with AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define S3 file paths for input and output data, the training image containing the semantic segmentation algorithm, and instantiate a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "training_image = retrieve(region=sess.boto_region_name, framework=\"object-detection\", version=\"latest\")\n",
    "\n",
    "bucket = 'gov-agm-temp'\n",
    "prefix = 'obj_detect'\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Inputs\n",
    "\n",
    "*Be sure to edit the file names and paths below for your own use!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_manifest_filename_train = (\n",
    "    \"training_manifest_with_validation_mod.json\"  # Replace with the filename for your training data.\n",
    ")\n",
    "augmented_manifest_filename_validation = (\n",
    "    \"testing_manifest_with_validation_mod.json\"  # Replace with the filename for your validation data.\n",
    ")\n",
    "bucket_name = \"ml-materials\"  # Replace with your bucket name.\n",
    "s3_prefix = \"object_detection_dataset/labeled_data/LabelJob1/manifests/output\"  # Replace with the S3 prefix where your data files reside.\n",
    "s3_output_path = \"s3://{}/output\".format(bucket_name)  # Replace with your desired output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup section concludes with a few more definitions and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines paths for use in the training job request.\n",
    "s3_train_data_path = \"s3://{}/{}/{}\".format(\n",
    "    bucket_name, s3_prefix, augmented_manifest_filename_train\n",
    ")\n",
    "s3_validation_data_path = \"s3://{}/{}/{}\".format(\n",
    "    bucket_name, s3_prefix, augmented_manifest_filename_validation\n",
    ")\n",
    "\n",
    "print(\"Augmented manifest for training data: {}\".format(s3_train_data_path))\n",
    "print(\"Augmented manifest for validation data: {}\".format(s3_validation_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Augmented Manifest format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented manifests provide two key benefits. First, the format is consistent with that of a labeling job output manifest. This means that you can take your output manifests from a Ground Truth labeling job and, whether the dataset objects were entirely human-labeled, entirely machine-labeled, or anything in between, and use them as inputs to SageMaker training jobs - all without any additional translation or reformatting! Second, the dataset objects and their corresponding ground truth labels/annotations are captured *inline*. This effectively reduces the required number of channels by half, since you no longer need one channel for the dataset objects alone and another for the associated ground truth labels/annotations.\n",
    "\n",
    "The augmented manifest format is essentially the [json-lines format](http://jsonlines.org/), also called the new-line delimited JSON format. This format consists of an arbitrary number of well-formed, fully-defined JSON objects, each on a separate line. Augmented manifests must contain a field that defines a dataset object, and a field that defines the corresponding annotation. Let's look at an example for an object detection problem.\n",
    "\n",
    "The Ground Truth output format is discussed more fully for various types of labeling jobs in the [official documenation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{<span style=\"color:blue\">\"source-ref\"</span>: \"s3://bucket_name/path_to_a_dataset_object.jpeg\", <span style=\"color:blue\">\"labeling-job-name\"</span>: {\"annotations\":[{\"class_id\":\"0\",`<bounding box dimensions>`}],\"image_size\":[{`<image size simensions>`}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first field will always be either `source` our `source-ref`. This defines an individual dataset object. The name of the second field depends on whether the labeling job was created from the SageMaker console or through the Ground Truth API. If the job was created through the console, then the name of the field will be the labeling job name. Alternatively, if the job was created through the API, then this field maps to the `LabelAttributeName` parameter in the API. \n",
    "\n",
    "The training job request requires a parameter called `AttributeNames`. This should be a two-element list of strings, where the first string is \"source-ref\", and the second string is the label attribute name from the augmented manifest. This corresponds to the <span style=\"color:blue\">blue text</span> in the example above. In this case, we would define `attribute_names = [\"source-ref\", \"labeling-job-name\"]`.\n",
    "\n",
    "*Be sure to carefully inspect your augmented manifest so that you can define the `attribute_names` variable below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Input Data\n",
    "\n",
    "Let's read the augmented manifest so we can inspect its contents to better understand the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_manifest_s3_key = s3_train_data_path.split(bucket_name)[1][1:]\n",
    "s3_obj = s3.Object(bucket_name, augmented_manifest_s3_key)\n",
    "augmented_manifest = s3_obj.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "augmented_manifest_lines = augmented_manifest.split(\"\\n\")\n",
    "\n",
    "num_training_samples = len(\n",
    "    augmented_manifest_lines\n",
    ")  # Compute number of training samples for use in training job request.\n",
    "\n",
    "def json_pretty_print(jsonline):\n",
    "    return json.dumps(json.loads(jsonline),indent=2)\n",
    "\n",
    "print(\"Preview of Augmented Manifest File Contents\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Line {}\".format(i + 1))\n",
    "    print(json_pretty_print(augmented_manifest_lines[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key feature of the augmented manifest is that it has both the data object itself (i.e., the image), and the annotation in-line in a single JSON object. Note that the `annotations` keyword contains dimensions and coordinates (e.g., width, top, height, left) for bounding boxes! The augmented manifest can contain an arbitrary number of lines, as long as each line adheres to this format.\n",
    "\n",
    "Let's discuss this format in more detail by descibing each parameter of this JSON object format.\n",
    "\n",
    "* The `source-ref` field defines a single dataset object, which in this case is an image over which bounding boxes should be drawn. Note that the name of this field is arbitrary. \n",
    "* The `object-detection-job-name` field defines the ground truth bounding box annotations that pertain to the image identified in the `source-ref` field. As mentioned above, note that the name of this field is arbitrary. You must take care to define this field in the `AttributeNames` parameter of the training job request, as shown later on in this notebook.\n",
    "* Because this example augmented manifest was generated through a Ground Truth labeling job, this example also shows an additional field called `object-detection-job-name-metadata`. This field contains various pieces of metadata from the labeling job that produced the bounding box annotation(s) for the associated image, e.g., the creation date, confidence scores for the annotations, etc. This field is ignored during the training job. However, to make it as easy as possible to translate Ground Truth labeling jobs into trained SageMaker models, it is safe to include this field in the augmented manifest you supply to the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names = [\"source-ref\", \"nugdms3-train_BB\"]  # Replace as appropriate for your augmented manifest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Job\n",
    "\n",
    "First, we'll construct the request for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if attribute_names == [\"source-ref\", \"XXXX\"]:\n",
    "        raise Exception(\n",
    "            \"The 'attribute_names' variable is set to default values. Please check your augmented manifest file for the label attribute name and set the 'attribute_names' variable accordingly.\"\n",
    "        )\n",
    "except NameError:\n",
    "    raise Exception(\n",
    "        \"The attribute_names variable is not defined. Please check your augmented manifest file for the label attribute name and set the 'attribute_names' variable accordingly.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the Amazon SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.8xlarge\",\n",
    "    volume_size=50,\n",
    "    max_run=360000,\n",
    "    input_mode=\"Pipe\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "od_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=3,\n",
    "    mini_batch_size=32,\n",
    "    epochs=200,\n",
    "    learning_rate=0.01,\n",
    "    optimizer=\"sgd\",\n",
    "    image_shape=512,\n",
    "    num_training_samples=str(num_training_samples),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "train_data = TrainingInput(\n",
    "    s3_train_data_path,    \n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    attribute_names=attribute_names\n",
    ")\n",
    "\n",
    "validation_data = TrainingInput(\n",
    "    s3_validation_data_path,    \n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    attribute_names=attribute_names\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "od_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "object_detector = od_model.deploy(\n",
    "                    initial_instance_count = 1, \n",
    "                    instance_type = 'ml.m4.xlarge', \n",
    "                    serializer = sagemaker.serializers.IdentitySerializer('image/png'),\n",
    "                    deserializer = sagemaker.deserializers.JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first define some functions to help visualize the results<br>\n",
    "First of all, in machine learning you're dealing with a numeric matrix and the predictions for the labels are the numeric labels assigned when we provided the label data<br>\n",
    "i.e.<br>\n",
    "    0   -   Person<br>\n",
    "    1   -   StackedBoxes<br>\n",
    "    2   -   ForkLift<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the mappings from label number to label\n",
    "object_categories = ['Person', 'StackedBoxes', 'Forklift']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a set of utilities to help visualize the results. All contained in the included visualize_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are loading  some utilities to help use visualize results from the predictions\n",
    "import visualize_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run a prediction against the endpoint and see the results (JSON output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('ml-materials')\n",
    "img = bucket.Object(f'object_detection_dataset/source_images/image_0000240.5.png').get().get('Body').read()\n",
    "object_detector.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON Results are as a list of lists.<br> Each list contains 6 comma seperated numbers<br>\n",
    "**Label<br>\n",
    "Confidence<br>\n",
    "BoundBox Top Left X (% of image width)<br>\n",
    "BoundBox Top Left Y (% of image height)<br>\n",
    "BoundBox Width (% of image width)<br>\n",
    "BoundBox Height (% of image height)**<br>\n",
    "<br>As you can see you get a lot of results with a vast range of confidence for each. So we need to filter out the ones we are interested. We do this by defining a threshold (e.g. only show entries with confidence > 0.2) <br>\n",
    "Lets view a few images and the predictions with a threshold of 0.2 (>20% confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_photos = ['image_0000112.0.png','image_0000130.0.png','image_0000140.5.png','image_0000231.5.png']\n",
    "test_photos = ['image_0000212.0.png','image_0000230.0.png','image_0000240.5.png','image_0000241.5.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_utils.predictions_image_list(object_detector, test_photos, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a different function to view image ranges with the same threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_seq_list = [\"197-217\",\"235-249\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_utils.predictions_image_sequences(object_detector, video_seq_list, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now view the output predictions (that you would be analyzing across time sequences to assess the action being viewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the video frame sequence list we defined earlier. Before we run any predictions, lets make sure the image exists or find one near by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = []\n",
    "for times in video_seq_list:\n",
    "    tsplit = times.split('-')\n",
    "    tstart = tsplit[0]\n",
    "    tend = tsplit[1]\n",
    "    for v in range(int(tstart),int(tend),2):\n",
    "        fname = f'image_{v:07}.0.png'\n",
    "        checkfile = visualize_utils.s3_objexist('ml-materials',f'object_detection_dataset/source_images/{fname}')\n",
    "        if not checkfile:\n",
    "            fname = f'image_{v:07}.5.png'\n",
    "            checkfile = visualize_utils.s3_objexist('ml-materials',f'object_detection_dataset/source_images/{fname}')\n",
    "        print(f\"s3://ml-materials/object_detection_dataset/source_images/{fname} {checkfile}\")\n",
    "        na.append(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of images that do exist, let do a prediction and view the results<br>\n",
    "The verbalize_results function will run the prediction and output the the interpreted label name, absolute coordinates of the bounding box and confidence of the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in na:\n",
    "    results, detection_filtered = visualize_utils.only_predict(image, object_detector, threshold=0.2)\n",
    "    print(image)\n",
    "    visualize_utils.verbalize_results(image, results, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That's it! Let's review what we've learned. \n",
    "* Augmented manifests are a new format that provide a seamless interface between Ground Truth labeling jobs and SageMaker training jobs. \n",
    "* In augmented manifests, you specify the dataset objects and the associated annotations in-line.\n",
    "* Be sure to pay close attention to the `AttributeNames` parameter in the training job request. The strings you specifuy in this field must correspond to those that are present in your augmented manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
