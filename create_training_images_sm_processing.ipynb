{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Object Detection in GovCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDMS USE Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI/ML use case is to be able to process video from cameras mounted in the autoclaves and to be able to recognize loading and unloading of Radomes. To be able to do this, we will need to do the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./GDMS-obj-detection-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Image Extraction - We will extract image frames from an uploaded video.\n",
    " - *Image Labeling* - We will not be doing the labeling step as GroundTruth labeling service is not currently available in GovCloud. So for our workshop will use pre-labelled images\n",
    " - Train Object Detection Model - train an Object Detection builtin algorithm in SageMaker\n",
    " - Deploy to and Endpoint - Deploy the trained model to an endpoint\n",
    " - Perform Predictions -  do predictions against the endpoint and list the objects detected\n",
    " \n",
    " The initial approach was to use an AI Service called Rekognition Custom Labels, however this is not yet available in GovCloud so this approach will use SageMaker Object Detection instead. Lets see how the two apporaches would have differed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./obj-detection-complete-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the flow is not too much different when using Rekognition vs SageMaker. <br> So lets first concentrate on being able to split a large video into images before we can perform image labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Video into Images using a basic SageMaker Processing Script\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Object Detection requires images to be extracted from a video file. The images then need to be labelled with bounding boxes are each of the objects in each image. Once this process is complete, the builtin algorithm for Object Detection in SageMaker will be able to take the images and labels and start training an object detection model.\n",
    "\n",
    "![](./stage1-gov-obj-detect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a very basic example of using SageMaker Processing to create images from a video file to create an image dataset. SageMaker Processing is used to create this dataset, which then are written back to S3.\n",
    "\n",
    "First, letâ€™s create an SKLearnProcessor object, passing the scikit-learn version we want to use, as well as our managed infrastructure requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "import json\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "staging_bucket = 'PUT YOUR STAGING BUCKET NAME HERE'\n",
    "staging_prefix = 'input_data'\n",
    "\n",
    "output_bucket = 'PUT YOUR OUTPUT BUCKET NAME HERE'\n",
    "output_prefix = 'training_images'\n",
    "\n",
    "source_video = 'Loading_Trucks_At_The_Warehouse.mp4'\n",
    "config_file = 'config_file.json'\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the video to your staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://ml-materials/loading_warehouse/Loading_Trucks_At_The_Warehouse.mp4 to s3://agm-temp/input_data/Loading_Trucks_At_The_Warehouse.mp4\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://ml-materials/loading_warehouse/Loading_Trucks_At_The_Warehouse.mp4 s3://{staging_bucket}/{staging_prefix}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a config file that will get the attributes of how we want to analyze the video file to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'video_creation_time':'2022-01-29 08:00:00',\n",
    "    'capture_start_time': '2022-01-29 08:00:50',\n",
    "    'capture_end_time': '2022-01-29 08:04:30',\n",
    "    'capture_interval_in_seconds': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save the config file locally and transfer it to your staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_file.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./config_file.json to s3://agm-temp/input_data/config_file.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp \"config_file.json\" s3://{staging_bucket}/{staging_prefix}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python file on the local file system which will be used by SageMaker Processing.  \n",
    "This is the code that actually does the work of extracting images from the video file.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import gc\n",
    "\n",
    "# Install some libraries that we are going to use for image extraction and formatting\n",
    "os.system('pip3 install decord Pillow')\n",
    "\n",
    "import decord as de\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def convert_video_to_images(input_filename, config_file):\n",
    "    # Setup the path to the video and config file that SageMaker Processing maps locally\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", input_filename)\n",
    "    input_config_path = os.path.join(\"/opt/ml/processing/input\", config_file)\n",
    "\n",
    "    print(f\"\\n\\nLoading Config File {input_config_path}\")\n",
    "    \n",
    "    # Load config file\n",
    "    with open(input_config_path, \"r\") as cfd:\n",
    "        job_config = json.load(cfd)\n",
    "    \n",
    "    # Convert the attributes fron the config file into the values we will use for this job\n",
    "    cfg_video_base_time = dt.datetime.strptime(job_config['video_creation_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    cfg_start_time = dt.datetime.strptime(job_config['capture_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    cfg_end_time = dt.datetime.strptime(job_config['capture_end_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    interval_time = float(job_config['capture_interval_in_seconds'])\n",
    "    start_time_in_secs = (cfg_start_time - cfg_video_base_time).total_seconds()\n",
    "    end_time_in_secs = (cfg_end_time - cfg_video_base_time).total_seconds()\n",
    "    \n",
    "    print(f\"Reading Video File {input_data_path}\\n\")\n",
    "    \n",
    "    fid=open(input_data_path, 'rb')\n",
    "    vrd = de.VideoReader(input_data_path, width=512, height=384)\n",
    "    print('Video frames #:', len(vrd))\n",
    "    print('First frame shape:', vrd[0].shape)\n",
    "    fps_vid = int(vrd.get_avg_fps())\n",
    "    # Split data set into training, validation, and test\n",
    "    start_frame_number = int(start_time_in_secs*fps_vid)\n",
    "    start_frame = vrd[start_frame_number].asnumpy()\n",
    "    end_frame_number = int(end_time_in_secs*fps_vid)\n",
    "    end_frame = vrd[end_frame_number].asnumpy()\n",
    "    num_frames = vrd._num_frame\n",
    "    frame_list = []\n",
    "    frame_sample_interval = interval_time\n",
    "    frame_interval = int(frame_sample_interval * fps_vid)\n",
    "    frame_req_block = frame_interval * 15\n",
    "    \n",
    "    job_total_images = 0\n",
    "    \n",
    "    for y in range(start_frame_number,end_frame_number,frame_req_block):\n",
    "        block_list=[]\n",
    "        for x in range(y,y+frame_req_block,frame_interval):\n",
    "            block_list.append(x)\n",
    "            job_total_images += 1\n",
    "        frame_list.append(block_list)\n",
    "\n",
    "    output_prefix = '/opt/ml/processing/output'\n",
    "    image_count = 0\n",
    "\n",
    "    print(f\"\\nGenerating {job_total_images} Images ...\")\n",
    "    print(f\"Completed 0% ...\")\n",
    "    for block in frame_list:\n",
    "        image_frames = vrd.get_batch(block).asnumpy()\n",
    "        num_sec = float(block[0]/fps_vid)\n",
    "        for images in image_frames: \n",
    "            buffer = BytesIO()\n",
    "            image_array = Image.fromarray(images)\n",
    "            # Resize Image to fit within 512x512 pixels and maintain aspect ratio\n",
    "            # and save new PNG image into buffer \n",
    "            scaled_image = ImageOps.contain(image_array,(512,512))\n",
    "            scaled_image.save(buffer, format=\"png\")\n",
    "            \n",
    "            # Use frame to seconds calculated to generate meaningful image filename \n",
    "            current_frame_ts = cfg_video_base_time + dt.timedelta(0,num_sec)\n",
    "            image_base_fname = datetime.fromtimestamp(current_frame_ts.timestamp()).strftime(\"%Y-%m-%d-%H:%M:%S.%f\")\n",
    "            image_base_fname = image_base_fname[0:21]\n",
    "            \n",
    "            # Write out buffer to image filename\n",
    "            with open(f'{output_prefix}/image_{image_base_fname}.png','wb') as imgfd:\n",
    "                imgfd.write(buffer.getvalue())\n",
    "                image_count += 1\n",
    "            num_sec += frame_sample_interval\n",
    "            \n",
    "            # Print completion percentage status\n",
    "            complete_percent = image_count/job_total_images*100\n",
    "            if (complete_percent % 10 == 0):\n",
    "                print(f\"Completed {int(complete_percent)}% ...\")\n",
    "                \n",
    "            # delete temporary image buffers and force garbage collect to keep memory footprint low  \n",
    "            del buffer\n",
    "            del scaled_image\n",
    "            del image_array\n",
    "            gc.collect()\n",
    "        # delete temporary image buffers and force garbage collect to keep memory footprint low\n",
    "        del image_frames\n",
    "        gc.collect()\n",
    "    fid.close()\n",
    "    return image_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--video-filename\", type=str, default=\"NONE\")\n",
    "    parser.add_argument(\"--config-file\", type=str, default=\"NONE\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_video_filename = args.video_filename\n",
    "    input_config_file = args.config_file\n",
    "    if input_video_filename == \"NONE\" or input_config_file == \"NONE\":\n",
    "        print(\"Must provide --video-filename and --config-file. Exiting\")\n",
    "        raise Exception(\"Must provide --video-filename and --config-file. Exiting\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\"Received arguments {args}\")\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print (f\"Started Processing Job at : {now}\")\n",
    "    \n",
    "    images_created = convert_video_to_images(input_video_filename, input_config_file)\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print (f\"Finished Processing Job at : {now}\")\n",
    "    \n",
    "    print(f\"Images Created {images_created}\")\n",
    "    print(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the SageMaker Processing job which will run the *preprocessing.py* code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-02-21-05-53-41-138\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://agm-temp/input_data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-650687152614/sagemaker-scikit-learn-2022-02-21-05-53-41-138/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://agm-temp/training_images', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........................\u001b[34mCollecting decord\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /miniconda3/lib/python3.7/site-packages (8.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.14.0 in /miniconda3/lib/python3.7/site-packages (from decord) (1.19.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: decord\u001b[0m\n",
      "\u001b[34mSuccessfully installed decord-0.6.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(config_file='config_file.json', video_filename='Loading_Trucks_At_The_Warehouse.mp4')\u001b[0m\n",
      "\u001b[34mStarted Processing Job at : 2022-02-21 05:57:54\u001b[0m\n",
      "\u001b[34mLoading Config File /opt/ml/processing/input/config_file.json\u001b[0m\n",
      "\u001b[34mReading Video File /opt/ml/processing/input/Loading_Trucks_At_The_Warehouse.mp4\u001b[0m\n",
      "\u001b[34mVideo frames #: 10455\u001b[0m\n",
      "\u001b[34mFirst frame shape: (384, 512, 3)\u001b[0m\n",
      "\u001b[34mGenerating 450 Images ...\u001b[0m\n",
      "\u001b[34mCompleted 0% ...\u001b[0m\n",
      "\u001b[34mCompleted 10% ...\u001b[0m\n",
      "\u001b[34mCompleted 20% ...\u001b[0m\n",
      "\u001b[34mCompleted 30% ...\u001b[0m\n",
      "\u001b[34mCompleted 40% ...\u001b[0m\n",
      "\u001b[34mCompleted 50% ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"./preprocessing.py\",\n",
    "    arguments = ['--video-filename', source_video, '--config-file', config_file],\n",
    "    inputs=[\n",
    "        ProcessingInput(source=f's3://{staging_bucket}/{staging_prefix}',\n",
    "                        destination='/opt/ml/processing/input')\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                         destination=f's3://{output_bucket}/{output_prefix}'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this process has finished you can go and inspect the extracted images in your S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion - First Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just accomplished how to convert a large video file to seperate images in a fully managed way by leveraging SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
