{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Video into Images using a basic SageMaker Processing Script\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Object Detection requires images to be extracted from a video file. The images then need to be labelled with bounding boxes are each of the objects in each image. Once this process is complete, the builtin algorithm for Object Detection in SageMaker will be able to take the images and labels and start training an object detection model.\n",
    "\n",
    "![](./stage1-gov-obj-detect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a very basic example of using SageMaker Processing to create images from a video file to create an image dataset. SageMaker Processing is used to create this dataset, which then are written back to S3.\n",
    "\n",
    "First, letâ€™s create an SKLearnProcessor object, passing the scikit-learn version we want to use, as well as our managed infrastructure requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "import json\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "#staging_bucket = 'PUT YOUR STAGING BUCKET NAME HERE'\n",
    "staging_bucket = 'am-obj-detect'\n",
    "staging_prefix = 'input_data'\n",
    "\n",
    "#output_bucket = 'PUT YOUR OUTPUT BUCKET NAME HERE'\n",
    "output_bucket = 'am-obj-detect'\n",
    "output_prefix = 'training_images2'\n",
    "\n",
    "source_video = 'Loading_Trucks_At_The_Warehouse.mp4'\n",
    "config_file = 'config_file.json'\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the video to your staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://ml-materials/loading_warehouse/Loading_Trucks_At_The_Warehouse.mp4 to s3://am-obj-detect/input_data/Loading_Trucks_At_The_Warehouse.mp4\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://ml-materials/loading_warehouse/Loading_Trucks_At_The_Warehouse.mp4 s3://{staging_bucket}/{staging_prefix}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a config file that will get the attributes of how we want to analyze the video file to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'video_creation_time':'2022-01-29 08:00:00',\n",
    "    'capture_start_time': '2022-01-29 08:00:50',\n",
    "    'capture_end_time': '2022-01-29 08:04:30',\n",
    "    'capture_interval_in_seconds': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save the config file locally and transfer it to your staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_file.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./config_file.json to s3://am-obj-detect/input_data/config_file.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp \"config_file.json\" s3://{staging_bucket}/{staging_prefix}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python file on the local file system which will be used by SageMaker Processing.  \n",
    "This is the code that actually does the work of extracting images from the video file.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import gc\n",
    "\n",
    "# Install some libraries that we are going to use for image extraction and formatting\n",
    "os.system('pip3 install decord Pillow')\n",
    "\n",
    "import decord as de\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def convert_video_to_images(input_filename, config_file):\n",
    "    # Setup the path to the video and config file that SageMaker Processing maps locally\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", input_filename)\n",
    "    input_config_path = os.path.join(\"/opt/ml/processing/input\", config_file)\n",
    "\n",
    "    print(f\"\\n\\nLoading Config File {input_config_path}\")\n",
    "    \n",
    "    # Load config file\n",
    "    with open(input_config_path, \"r\") as cfd:\n",
    "        job_config = json.load(cfd)\n",
    "    \n",
    "    # Convert the attributes fron the config file into the values we will use for this job\n",
    "    cfg_video_base_time = dt.datetime.strptime(job_config['video_creation_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    cfg_start_time = dt.datetime.strptime(job_config['capture_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    cfg_end_time = dt.datetime.strptime(job_config['capture_end_time'], '%Y-%m-%d %H:%M:%S')\n",
    "    interval_time = float(job_config['capture_interval_in_seconds'])\n",
    "    start_time_in_secs = (cfg_start_time - cfg_video_base_time).total_seconds()\n",
    "    end_time_in_secs = (cfg_end_time - cfg_video_base_time).total_seconds()\n",
    "    \n",
    "    print(f\"Reading Video File {input_data_path}\\n\")\n",
    "    \n",
    "    fid=open(input_data_path, 'rb')\n",
    "    vrd = de.VideoReader(input_data_path, width=512, height=384)\n",
    "    print('Video frames #:', len(vrd))\n",
    "    print('First frame shape:', vrd[0].shape)\n",
    "    fps_vid = int(vrd.get_avg_fps())\n",
    "    # Split data set into training, validation, and test\n",
    "    start_frame_number = int(start_time_in_secs*fps_vid)\n",
    "    start_frame = vrd[start_frame_number].asnumpy()\n",
    "    end_frame_number = int(end_time_in_secs*fps_vid)\n",
    "    end_frame = vrd[end_frame_number].asnumpy()\n",
    "    num_frames = vrd._num_frame\n",
    "    frame_list = []\n",
    "    frame_sample_interval = interval_time\n",
    "    frame_interval = int(frame_sample_interval * fps_vid)\n",
    "    frame_req_block = frame_interval * 15\n",
    "    \n",
    "    job_total_images = 0\n",
    "    \n",
    "    for y in range(start_frame_number,end_frame_number,frame_req_block):\n",
    "        block_list=[]\n",
    "        for x in range(y,y+frame_req_block,frame_interval):\n",
    "            block_list.append(x)\n",
    "            job_total_images += 1\n",
    "        frame_list.append(block_list)\n",
    "\n",
    "    output_prefix = '/opt/ml/processing/output'\n",
    "    image_count = 0\n",
    "\n",
    "    print(f\"\\nGenerating {job_total_images} Images ...\")\n",
    "    print(f\"Completed 0% ...\")\n",
    "    for block in frame_list:\n",
    "        image_frames = vrd.get_batch(block).asnumpy()\n",
    "        num_sec = float(block[0]/fps_vid)\n",
    "        for images in image_frames: \n",
    "            buffer = BytesIO()\n",
    "            image_array = Image.fromarray(images)\n",
    "            # Resize Image to fit within 512x512 pixels and maintain aspect ratio\n",
    "            # and save new PNG image into buffer \n",
    "            scaled_image = ImageOps.contain(image_array,(512,512))\n",
    "            scaled_image.save(buffer, format=\"png\")\n",
    "            \n",
    "            # Use frame to seconds calculated to generate meaningful image filename \n",
    "            current_frame_ts = cfg_video_base_time + dt.timedelta(0,num_sec)\n",
    "            image_base_fname = datetime.fromtimestamp(current_frame_ts.timestamp()).strftime(\"%Y-%m-%d-%H:%M:%S.%f\")\n",
    "            image_base_fname = image_base_fname[0:21]\n",
    "            \n",
    "            # Write out buffer to image filename\n",
    "            with open(f'{output_prefix}/image_{image_base_fname}.png','wb') as imgfd:\n",
    "                imgfd.write(buffer.getvalue())\n",
    "                image_count += 1\n",
    "            num_sec += frame_sample_interval\n",
    "            \n",
    "            # Print completion percentage status\n",
    "            complete_percent = image_count/job_total_images*100\n",
    "            if (complete_percent % 10 == 0):\n",
    "                print(f\"Completed {int(complete_percent)}% ...\")\n",
    "                \n",
    "            # delete temporary image buffers and force garbage collect to keep memory footprint low  \n",
    "            del buffer\n",
    "            del scaled_image\n",
    "            del image_array\n",
    "            gc.collect()\n",
    "        # delete temporary image buffers and force garbage collect to keep memory footprint low\n",
    "        del image_frames\n",
    "        gc.collect()\n",
    "    fid.close()\n",
    "    return image_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--video-filename\", type=str, default=\"NONE\")\n",
    "    parser.add_argument(\"--config-file\", type=str, default=\"NONE\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_video_filename = args.video_filename\n",
    "    input_config_file = args.config_file\n",
    "    if input_video_filename == \"NONE\" or input_config_file == \"NONE\":\n",
    "        print(\"Must provide --video-filename and --config-file. Exiting\")\n",
    "        raise Exception(\"Must provide --video-filename and --config-file. Exiting\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\"Received arguments {args}\")\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print (f\"Started Processing Job at : {now}\")\n",
    "    \n",
    "    images_created = convert_video_to_images(input_video_filename, input_config_file)\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print (f\"Finished Processing Job at : {now}\")\n",
    "    \n",
    "    print(f\"Images Created {images_created}\")\n",
    "    print(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the SageMaker Processing job which will run the *preprocessing.py* code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2022-02-17-21-30-24-665\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://am-obj-detect/input_data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-662559257807/sagemaker-scikit-learn-2022-02-17-21-30-24-665/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://am-obj-detect/training_images2', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........................\u001b[34mCollecting decord\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting Pillow\n",
      "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.14.0 in /miniconda3/lib/python3.7/site-packages (from decord) (1.19.5)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: Pillow, decord\u001b[0m\n",
      "\u001b[34mSuccessfully installed Pillow-9.0.1 decord-0.6.0\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(config_file='config_file.json', video_filename='Loading_Trucks_At_The_Warehouse.mp4')\u001b[0m\n",
      "\u001b[34mStarted Processing Job at : 2022-02-17 21:34:49\u001b[0m\n",
      "\u001b[34mLoading Config File /opt/ml/processing/input/config_file.json\u001b[0m\n",
      "\u001b[34mReading Video File /opt/ml/processing/input/Loading_Trucks_At_The_Warehouse.mp4\u001b[0m\n",
      "\u001b[34mVideo frames #: 10455\u001b[0m\n",
      "\u001b[34mFirst frame shape: (384, 512, 3)\u001b[0m\n",
      "\u001b[34mGenerating 450 Images ...\u001b[0m\n",
      "\u001b[34mCompleted 0% ...\u001b[0m\n",
      "\u001b[34mCompleted 10% ...\u001b[0m\n",
      "\u001b[34mCompleted 20% ...\u001b[0m\n",
      "\u001b[34mCompleted 30% ...\u001b[0m\n",
      "\u001b[34mCompleted 40% ...\u001b[0m\n",
      "\u001b[34mCompleted 50% ...\u001b[0m\n",
      "\u001b[34mCompleted 60% ...\u001b[0m\n",
      "\u001b[34mCompleted 70% ...\u001b[0m\n",
      "\u001b[34mCompleted 80% ...\u001b[0m\n",
      "\u001b[34mCompleted 90% ...\u001b[0m\n",
      "\u001b[34mCompleted 100% ...\u001b[0m\n",
      "\u001b[34mFinished Processing Job at : 2022-02-17 21:36:04\u001b[0m\n",
      "\u001b[34mImages Created 450\u001b[0m\n",
      "\u001b[34mFinished running processing job\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"./preprocessing.py\",\n",
    "    arguments = ['--video-filename', source_video, '--config-file', config_file],\n",
    "    inputs=[\n",
    "        ProcessingInput(source=f's3://{staging_bucket}/{staging_prefix}',\n",
    "                        destination='/opt/ml/processing/input')\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                         destination=f's3://{output_bucket}/{output_prefix}'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
